---
permalink: /guides/app-logging/
---
:projectid: app-logging
:page-layout: guide
:page-duration: 30 minutes
:page-releasedate: 2019-09-03
:page-guide-category: basic
:page-essential: true
:page-essential-order: 1
:page-description: Learn how to do application logging with Kabanero on OKD with Elasticsearch, FluentD, and Kibana.
:guide-author: Kabanero
:page-tags: ['logging', 'Elasticsearch', 'FluentD', 'Kibana']
:repo-description: Visit the https://kabanero.io/guides/{projectid}.html[website] for the rendered version of the guide.
= Application Logging with Kabanero on OKD with Elasticsearch, FluentD, and Kibana

__The following guide has been tested with OKD 4.2/Kabanero 0.3.0.__


Pod processes running in Kubernetes frequently produce logs. To effectively manage this log data and ensure no loss of log data occurs when a pod terminates, a log aggregation tool should be deployed on the Kubernetes cluster. Log aggregation tools help users persist, search, and visualize the log data that is gathered from the pods across the cluster. Log aggregation tools in the market today include:  EFK, LogDNA, Splunk, Datadog, IBM Operations Analytics, etc.  When considering log aggregation tools, enterprises will make choices that are inclusive of their journey to cloud, both new cloud native applications running in Kubernetes and their existing traditional IT choices.

One choice for application logging with log aggregation, based on open source, is **EFK (Elasticsearch, FluentD, and Kibana)**. This guide describes the process of deploying EFK using the Elasticsearch Operator and the Cluster Logging Operator introduced in OCP 4.1. Use this preconfigured EFK stack to aggregate all container logs. After a successful installation, the EFK pods should reside inside the *openshift-logging* namespace of the OCP cluster.

== Install Cluster Logging

To install Cluster Logging component, follow the Openshift guide https://docs.openshift.com/container-platform/4.2/logging/cluster-logging-eventrouter.html[Deploying cluster logging]

After the installation completes without any error, you can see the following pods that are running in the *openshift-logging* namespace. The exact number of pods running for each of the EFK component can vary depending on the configuration specified in the ClusterLogging CRD.

[source,role="no_copy"]
----
[root@rhel7-ocp ~]# oc get pods -n openshift-logging

NAME                                            READY   STATUS      RESTARTS   AGE
cluster-logging-operator-874597bcb-qlmlf        1/1     Running     0          150m
curator-1578684600-2lgqp                        0/1     Completed   0          4m46s
elasticsearch-cdm-4qrvthgd-1-5444897599-7rqx8   2/2     Running     0          9m6s
elasticsearch-cdm-4qrvthgd-2-865c6b6d85-69b4r   2/2     Running     0          8m3s
fluentd-rmdbn                                   1/1     Running     0          9m5s
fluentd-vtk48                                   1/1     Running     0          9m5s
kibana-756fcdb7f-rw8k8                          2/2     Running     0          9m6s
----

The cluster logging also exposes a routes for external access to the Kibana console.

[source,role="no_copy"]
----
[root@rhel7-okd ~]# oc get routes -n openshift-logging

NAME     HOST/PORT                                  PATH   SERVICES   PORT    TERMINATION          WILDCARD
kibana   host.kabanero.com                          kibana     <all>   reencrypt/Redirect   None
----

== Configure to merge JSON log message body

If there are application pods outputting logs in JSON format, then it is best practice to set FluentD to parse the JSON fields from the message body and merge the parsed object with the JSON payload document posted to Elasticsearch.

This feature is disabled by default. To enable this feature, first set the cluster logging instance's **managementState** field from **"Managed"** to **"Unmanaged"**.

[source,role="no_copy"]
----
[root@rhel7-ocp ~]# oc edit ClusterLogging instance

apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogging"
metadata:
  name: "instance"

....

spec:
  managementState: "Unmanaged"
----

Then, set the environment variable **MERGE_JSON_LOG** to **true** with the following command:

[source,role="no_copy"]
----
[root@rhel7-ocp ~]# oc set env ds/fluentd MERGE_JSON_LOG=true
----


== View application logs on Kibana

Before you use the **logging-kibana** for application logs, make sure that the application is already deployed in a namespace **NOT** from one of the ops namespaces. The ops namespaces are **default**, **openshift**, and **openshift-infra**.

In cases where the application server provides the option, output application logs in JSON format.  This will let you fully take advantage of Kibana's dashboard functions. Kibana is then able to process the data from each individual field of the JSON object to create customized visualizations for that field.

See the Kibana dashboard page by using the routes URL https://kibana.apps.9.37.135.153.nip.io. Log in using your OKD user and password, then the browser should redirect you to Kibana's **Discover** page where the newest logs of the selected index are being streamed. Select the **project.\*** index to view the application logs generated by the deployed application.

image::/img/guide/kibana_app.png[link="/img/guide/kibana_app.png" alt="Kibana page with the application log entries"]
*Figure 2: Kibana page with the application log entries*

The **project.\*** index contains only a set of default fields at the start, which does not include all of the fields from the deployed application's JSON log object. Therefore, the index needs to be refreshed to have all the fields from the application's log object available to Kibana.

To refresh the index, click on the **Management** option on the left pane.

Click **Index Pattern**, and find the **project.\***  index in Index Pattern. Then, click the refresh fields button, which is on the right. After Kibana is updated with all the available fields in the **project.\*** index, import the preconfigured dashboards to view the application logs.

image::/img/guide/refresh_index.png[link="/img/guide/refresh_index.png" alt="Index refresh button on Kibana"]
*Figure 3: Index refresh button on Kibana*

To import the dashboard and its associated objects, navigate back to the **Management** page and click **Saved Objects**. Click **Import** and select the dashboard file. When prompted, click the **Yes, overwrite all** option

Head back to the **Dashboard** page and enjoy navigating logs on the newly imported dashboard.

image::/img/guide/kibana_open_liberty_dashboard.png[link="/img/guide/kibana_open_liberty_dashboard.png" alt="Kibana dashboard for Open Liberty application logs"]
*Figure 4: Kibana dashboard for Open Liberty application logs*

== Reinstalling and uninstalling openshift-logging

If changes need to be made for the installed EFK stack, rerun the ansible-playbook installation command with updated ansible variables values in the inventory file. If the aggregated container's logging stack is no longer needed in the current cluster, you can use the same ansible-playbook command to uninstall the openshift-logging feature. Uninstall the feature by setting the **openshift_logging_install_logging** variable to False.
